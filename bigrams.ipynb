{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk.collocations as collocations\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('./movie_reviews/train.tsv', 'r')\n",
    "f = open('train.tsv', 'r')\n",
    "# loop over lines in the file and use the first limit of them\n",
    "phrasedata = []\n",
    "for line in f:\n",
    "  # ignore the first line starting with Phrase and read all lines\n",
    "  if (not line.startswith('Phrase')):\n",
    "    # remove final end of line character\n",
    "    line = line.strip()\n",
    "    # each line has 4 items separated by tabs\n",
    "    # ignore the phrase and sentence ids, and keep the phrase and sentiment\n",
    "    phrasedata.append(line.split('\\t')[2:4])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['since 1997', '2']\n",
      "['that undercuts its charm', '2']\n",
      "['suggest that the wayward wooden one end it all by stuffing himself into an electric pencil sharpener', '1']\n",
      "['a great writer and', '3']\n",
      "['saw this one', '2']\n",
      "['While the frequent allusions to gurus and doshas will strike some Westerners as verging on mumbo-jumbo ... broad streaks of common sense emerge with unimpeachable clarity .', '3']\n",
      "[\"-LRB- Washington 's -RRB-\", '2']\n",
      "['the film is superficial and will probably be of interest primarily to its target audience', '1']\n",
      "['will the actors generate', '2']\n",
      "['a heroine who comes across as both shallow and dim-witted', '1']\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata\n",
    "for phrase in phraselist[:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['since', '1997'], 2)\n",
      "(['that', 'undercuts', 'its', 'charm'], 2)\n",
      "(['suggest', 'that', 'the', 'wayward', 'wooden', 'one', 'end', 'it', 'all', 'by', 'stuffing', 'himself', 'into', 'an', 'electric', 'pencil', 'sharpener'], 1)\n",
      "(['a', 'great', 'writer', 'and'], 3)\n",
      "(['saw', 'this', 'one'], 2)\n",
      "(['While', 'the', 'frequent', 'allusions', 'to', 'gurus', 'and', 'doshas', 'will', 'strike', 'some', 'Westerners', 'as', 'verging', 'on', 'mumbo-jumbo', '...', 'broad', 'streaks', 'of', 'common', 'sense', 'emerge', 'with', 'unimpeachable', 'clarity', '.'], 3)\n",
      "(['-LRB-', 'Washington', \"'s\", '-RRB-'], 2)\n",
      "(['the', 'film', 'is', 'superficial', 'and', 'will', 'probably', 'be', 'of', 'interest', 'primarily', 'to', 'its', 'target', 'audience'], 1)\n",
      "(['will', 'the', 'actors', 'generate'], 2)\n",
      "(['a', 'heroine', 'who', 'comes', 'across', 'as', 'both', 'shallow', 'and', 'dim-witted'], 1)\n"
     ]
    }
   ],
   "source": [
    "for phrase in phrasedocs[:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['since', '1997'], 2)\n",
      "(['that', 'undercuts', 'its', 'charm'], 2)\n",
      "(['suggest', 'that', 'the', 'wayward', 'wooden', 'one', 'end', 'it', 'all', 'by', 'stuffing', 'himself', 'into', 'an', 'electric', 'pencil', 'sharpener'], 1)\n",
      "(['a', 'great', 'writer', 'and'], 3)\n",
      "(['saw', 'this', 'one'], 2)\n",
      "(['while', 'the', 'frequent', 'allusions', 'to', 'gurus', 'and', 'doshas', 'will', 'strike', 'some', 'westerners', 'as', 'verging', 'on', 'mumbo-jumbo', '...', 'broad', 'streaks', 'of', 'common', 'sense', 'emerge', 'with', 'unimpeachable', 'clarity', '.'], 3)\n",
      "(['-lrb-', 'washington', \"'s\", '-rrb-'], 2)\n",
      "(['the', 'film', 'is', 'superficial', 'and', 'will', 'probably', 'be', 'of', 'interest', 'primarily', 'to', 'its', 'target', 'audience'], 1)\n",
      "(['will', 'the', 'actors', 'generate'], 2)\n",
      "(['a', 'heroine', 'who', 'comes', 'across', 'as', 'both', 'shallow', 'and', 'dim-witted'], 1)\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "for phrase in docs[:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16537\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_items = all_words.most_common(1000)\n",
    "word_features = [word for (word, count) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-d1b16b106a7c>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  doc_chunks = np.array_split(np.array(docs), 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc_chunks = np.array_split(np.array(docs), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['since', '1997']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['since', '1997'], 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\"'30s\", 'friendships'), 6.047046018020197e-05),\n",
       " ((\"'40s\", 'stuffing'), 6.047046018020197e-05),\n",
       " ((\"'50s\", 'frustratingly'), 6.047046018020197e-05),\n",
       " ((\"'60s\", 'sensual'), 6.047046018020197e-05),\n",
       " ((\"'70s\", 'represents'), 6.047046018020197e-05),\n",
       " ((\"'80s\", 'brother-man'), 6.047046018020197e-05),\n",
       " ((\"'90s\", 'norm'), 6.047046018020197e-05),\n",
       " ((\"'d\", 'intelligence'), 6.047046018020197e-05),\n",
       " ((\"'em\", 'ambition'), 6.047046018020197e-05),\n",
       " ((\"'ll\", 'history'), 6.047046018020197e-05)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "import re\n",
    "\n",
    "#data cleaning and preprocessing\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def alpha(w):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if(pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#creating bigrams features for the corpus and applying cleaning steps\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words)\n",
    "finder.apply_word_filter(alpha)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'30s\", 'friendships'),\n",
       " (\"'40s\", 'stuffing'),\n",
       " (\"'50s\", 'frustratingly'),\n",
       " (\"'60s\", 'sensual'),\n",
       " (\"'70s\", 'represents'),\n",
       " (\"'80s\", 'brother-man'),\n",
       " (\"'90s\", 'norm'),\n",
       " (\"'d\", 'intelligence'),\n",
       " (\"'em\", 'ambition'),\n",
       " (\"'ll\", 'history'),\n",
       " (\"'m\", 'emotionally'),\n",
       " (\"'re\", 'feel'),\n",
       " (\"'til\", 'swiftly'),\n",
       " (\"'ve\", 'lot'),\n",
       " ('-lrb-', 'good'),\n",
       " ('10-course', 'banquet'),\n",
       " ('10-year', 'delay'),\n",
       " ('10-year-old', 'ontiveros'),\n",
       " ('100-minute', 'ok'),\n",
       " ('100-year', 'oversimplification'),\n",
       " ('102-minute', 'bacon'),\n",
       " ('103-minute', 'glimmer'),\n",
       " ('10th', 'porthole'),\n",
       " ('10th-grade', 'truckzilla'),\n",
       " ('112-minute', 'therapeutic'),\n",
       " ('12-year-old', 'secretary'),\n",
       " ('129-minute', 'rhames'),\n",
       " ('12th', 'shaw'),\n",
       " ('13th', 'pours'),\n",
       " ('14-year-old', 'non-fan')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting clean bigrams (no frequency information)\n",
    "bigram_features = [bigram for (bigram, count) in scored[:1000]]\n",
    "#printing the first 30 for confirmation\n",
    "bigram_features[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_document_features(document, bigram_features):\n",
    "    document_words = list(nltk.bigrams(document))\n",
    "    features = {}\n",
    "    for word in bigram_features:\n",
    "        #boolean logic will retunt 'True' if there is a match, or 'False' if not\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigram_chunks = []\n",
    "for chunk in doc_chunks:\n",
    "    bigram_chunks.append(\n",
    "    [(bi_document_features(chunk_sent[0], bigram_features), chunk_sent[1])\n",
    "    for chunk_sent in chunk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "len(list(itertools.chain.from_iterable(bigram_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "featuresets2 = list(itertools.chain.from_iterable(bigram_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the length of the featureset\n",
    "#it should be the same than unigram, because we processed number of documents\n",
    "#this is for verification\n",
    "len(featuresets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_naive_bayes(feature_sets, splits=10):\n",
    "    kf = KFold(n_splits=splits)\n",
    "    scores = []\n",
    "\n",
    "    for train, test in kf.split(feature_sets):\n",
    "        classifier = nltk.NaiveBayesClassifier.train(\n",
    "            np.array(feature_sets)[train]\n",
    "        )\n",
    "        scores.append(\n",
    "            nltk.classify.accuracy(classifier, \n",
    "                                   np.array(feature_sets)[test])\n",
    "        )\n",
    "        \n",
    "    return classifier, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(true_labels, predictions):\n",
    "    precision = precision_score(true_labels, predictions, average='macro').round(4)\n",
    "    recall = recall_score(true_labels, predictions, average='macro').round(4)\n",
    "    f_measure = f1_score(true_labels, predictions, average='macro').round(4)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F Measure: \", f_measure)\n",
    "    return precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uni_10fold_results2 = kfold_naive_bayes(featuresets2, splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uni_model_5fold.pickle', 'wb') as f:\n",
    "#with open(\"./models/uni_model_5fold.pickle\", 'wb') as f:\n",
    "    pickle.dump(uni_10fold_results2[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5099384851980008"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(uni_10fold_results2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uni_10fold_pres_labs = [(uni_10fold_results2[0].classify(features), label)\n",
    "                       for features, label in featuresets2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.4599\n",
      "Recall:  0.2002\n",
      "F Measure:  0.1356\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for pred, true in uni_10fold_pres_labs:\n",
    "    preds.append(pred)\n",
    "    trues.append(true)\n",
    "\n",
    "uni_10fold_pres_labs = [trues, preds]\n",
    "uni_10fold_scores = score_model(uni_10fold_pres_labs[0], uni_10fold_pres_labs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
